{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335rM4bdE0lF"
      },
      "source": [
        "# 0. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvTsnne60yEI"
      },
      "source": [
        "- 라이브러리 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aBb02rmmBo4a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRMJQ0Z5CsR"
      },
      "source": [
        "# 1. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4ydlENzFAeUm"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    'drink cold milk',\n",
        "    'drink cold water',\n",
        "    'drink cold cola',\n",
        "    'drink sweet juice',\n",
        "    'drink sweet cola',\n",
        "    'eat delicious bacon',\n",
        "    'eat sweet mango',\n",
        "    'eat delicious cherry',\n",
        "    'eat sweet apple',\n",
        "    'juice with sugar',\n",
        "    'cola with sugar',\n",
        "    'mango is fruit',\n",
        "    'apple is fruit',\n",
        "    'cherry is fruit',\n",
        "    'Berlin is Germany',\n",
        "    'Boston is USA',\n",
        "    'Mercedes from Germany',\n",
        "    'Mercedes is car',\n",
        "    'Ford from USA',\n",
        "    'Ford is a car'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLCt_xNZ4VzK"
      },
      "source": [
        "# 2. 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFixYYfB9mtH"
      },
      "source": [
        "## 2-1. 단어 리스트 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MKOmWp58QgR-"
      },
      "outputs": [],
      "source": [
        "word_sequence = [sequence.split() for sequence in data]\n",
        "word_list = list(set(\" \".join(data).split()))\n",
        "word2index = {key : idx for idx, key in enumerate(word_list, start = 1)}\n",
        "word2index['<PAD>'] = 0\n",
        "vocab_size = len(word_list) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlpB00kI-SYj"
      },
      "source": [
        "# 2-2. Dataset 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZXzj6G-a85"
      },
      "source": [
        "#### Window Size 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e1qIVEnJQxHL"
      },
      "outputs": [],
      "source": [
        "window_size = 2\n",
        "batch_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SdL6_n9b3Al"
      },
      "source": [
        "### 2-2-1. CBOW Dataset 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFab12CtQ7iR"
      },
      "source": [
        "#### 페어 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1Ep8Gntlght"
      },
      "source": [
        "- CBOW_pair 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-GYu-UyijrXX"
      },
      "outputs": [],
      "source": [
        "def CBOW_pair(window_size, word_sequence) :\n",
        "    x_train, y_train = [], []\n",
        "    for sequence in word_sequence :\n",
        "        for i in range(len(sequence)) :\n",
        "            y_train.append(word2index[sequence[i]])\n",
        "            context = []\n",
        "            for j in range(window_size, 0, -1) :\n",
        "                # 과거 문자 삽입(With padding)\n",
        "                context.append(word2index[sequence[i-j]] if i - j >= 0 else 0)\n",
        "            for j in range(1, window_size + 1) :\n",
        "                # 미래 문자 삽입(With padding)\n",
        "                context.append(word2index[sequence[i+j]] if i + j < len(sequence) else 0)\n",
        "            # 페어 생성\n",
        "            x_train.append(context)\n",
        "    return torch.LongTensor(x_train), torch.LongTensor(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5c3l37AF7Gqn"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = CBOW_pair(window_size, word_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf2cIrTw-gsu"
      },
      "source": [
        "- DataLoader 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f_jqFa5Q4ng3"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2SFUeO-mxiz"
      },
      "source": [
        "# 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyioJsjvm6ui"
      },
      "source": [
        "## 3-1. CBOW Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Bgn4jpq5mcNa"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module) :\n",
        "\n",
        "    def __init__(self, vocab_size, dimension_size) :\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, dimension_size)\n",
        "        self.linear = nn.Linear(dimension_size, vocab_size, bias = False)\n",
        "        self.activation = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, X) :\n",
        "        X = self.embeddings(X)\n",
        "        X = X.sum(dim = 1)\n",
        "        X = self.linear(X)\n",
        "        X = self.activation(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHTs5U3T9xC4"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTgjlJrbAN5Y"
      },
      "source": [
        "## 4-1. CBOW Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URpmZsXps4Yv"
      },
      "source": [
        "- parameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NKRQ6CeSH3LM"
      },
      "outputs": [],
      "source": [
        "dimension_size = 5\n",
        "epochs = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-jhxjNGs6QR"
      },
      "source": [
        "- Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BImeDBWgnnhR",
        "outputId": "fc0df8c1-92e7-497b-a954-3228a7a6be58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0, loss : 4.161\n",
            "epoch : 100, loss : 1.276\n",
            "epoch : 200, loss : 2.320\n",
            "epoch : 300, loss : 1.113\n",
            "epoch : 400, loss : 1.482\n",
            "epoch : 500, loss : 1.874\n",
            "epoch : 600, loss : 1.463\n",
            "epoch : 700, loss : 1.934\n",
            "epoch : 800, loss : 0.908\n",
            "epoch : 900, loss : 1.025\n",
            "epoch : 1000, loss : 0.255\n"
          ]
        }
      ],
      "source": [
        "model = CBOW(vocab_size, dimension_size)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(epochs + 1) :\n",
        "    for feature, label in train_dataloader :\n",
        "        out = model(feature)\n",
        "        loss = criterion(out, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 100 == 0 : print(\"epoch : {:d}, loss : {:0.3f}\".format(i, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPGx5p7Utwdw"
      },
      "source": [
        "# 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XPnpfs93KQBL"
      },
      "outputs": [],
      "source": [
        "index2word = {value : key for key, value in word2index.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKvgHjfzKR9y"
      },
      "source": [
        "## 5-1. 단어 유사도 측정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raWOciWkKZON"
      },
      "source": [
        "### 5-1-1. 유사 단어 상위 3개 추출 함수 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_GeqURfUKWPK"
      },
      "outputs": [],
      "source": [
        "def find_similarity(target_word) :\n",
        "    target_word_embed = model.state_dict()['embeddings.weight'][word2index[target_word]]\n",
        "\n",
        "    similarity = []\n",
        "    for i in range(len(word2index)):\n",
        "        if target_word != index2word[i]:\n",
        "            similarity.append(( i, F.cosine_similarity(target_word_embed.unsqueeze(0), model.state_dict()['embeddings.weight'][i].unsqueeze(0)).item()))\n",
        "        else:\n",
        "            similarity.append((i, -1)) # target_word와 동일 단어는 -1 처리\n",
        "\n",
        "    # 유사도 내림차순 정렬\n",
        "    similarity.sort(key = lambda x : -x[1])\n",
        "\n",
        "    # 인덱스를 단어로 변환\n",
        "    print(f'{target_word}와 유사한 단어:')\n",
        "    for i in range(3) :\n",
        "        print(f'{i+1}위 : {index2word[similarity[i][0]]}({similarity[i][1]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aWZ-G9eKq4A"
      },
      "source": [
        "### 5-2-2. 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZgKqIiJKtI0",
        "outputId": "9efd3e11-4260-4ab3-ddbd-aa7afe8874f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cola와 유사한 단어:\n",
            "1위 : milk(0.7629103064537048)\n",
            "2위 : juice(0.727013111114502)\n",
            "3위 : eat(0.511882483959198)\n"
          ]
        }
      ],
      "source": [
        "find_similarity(\"cola\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
