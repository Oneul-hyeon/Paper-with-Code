{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335rM4bdE0lF"
      },
      "source": [
        "# 0. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvTsnne60yEI"
      },
      "source": [
        "- 라이브러리 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aBb02rmmBo4a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRMJQ0Z5CsR"
      },
      "source": [
        "# 1. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4ydlENzFAeUm"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    'drink cold milk',\n",
        "    'drink cold water',\n",
        "    'drink cold cola',\n",
        "    'drink sweet juice',\n",
        "    'drink sweet cola',\n",
        "    'eat delicious bacon',\n",
        "    'eat sweet mango',\n",
        "    'eat delicious cherry',\n",
        "    'eat sweet apple',\n",
        "    'juice with sugar',\n",
        "    'cola with sugar',\n",
        "    'mango is fruit',\n",
        "    'apple is fruit',\n",
        "    'cherry is fruit',\n",
        "    'Berlin is Germany',\n",
        "    'Boston is USA',\n",
        "    'Mercedes from Germany',\n",
        "    'Mercedes is car',\n",
        "    'Ford from USA',\n",
        "    'Ford is car'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLCt_xNZ4VzK"
      },
      "source": [
        "# 2. 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFixYYfB9mtH"
      },
      "source": [
        "## 2-1. 단어 리스트 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MKOmWp58QgR-"
      },
      "outputs": [],
      "source": [
        "word_sequence = [sequence.split() for sequence in data]\n",
        "word_list = list(set(\" \".join(data).split()))\n",
        "word2index = {key : idx for idx, key in enumerate(word_list) if len(key) > 1}\n",
        "vocab_size = len(word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlpB00kI-SYj"
      },
      "source": [
        "# 2-2. Dataset 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZXzj6G-a85"
      },
      "source": [
        "#### Window Size 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "e1qIVEnJQxHL"
      },
      "outputs": [],
      "source": [
        "window_size = 2\n",
        "batch_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNjlYCM8l7g2"
      },
      "source": [
        "### 2-2-1. Skip-gram Dataset 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFab12CtQ7iR"
      },
      "source": [
        "### 페어 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI-fACS5mQZ1"
      },
      "source": [
        "- SkipGram_pair 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "i1GaCb38lo-c"
      },
      "outputs": [],
      "source": [
        "def SkipGram_pair(window_size, word_sequence) :\n",
        "    pair = []\n",
        "    for sequence in word_sequence :\n",
        "        for i in range(len(sequence)) :\n",
        "            context = word2index[sequence[i]]\n",
        "            target = []\n",
        "            for j in range(window_size, -1, -1) :\n",
        "                if i - j >= 0 : target.append(word2index[sequence[i-j]]) # 과거 문자 삽입\n",
        "            for j in range(1, window_size + 1) :\n",
        "                if i + j < len(sequence) : target.append(word2index[sequence[i+j]] ) # 미래 문자 삽입\n",
        "            # 페어 생성\n",
        "            for t in target : pair.append([context, t])\n",
        "\n",
        "    x_train, y_train = list(zip(*pair))\n",
        "    return torch.LongTensor(list(x_train)), torch.LongTensor(list(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "u0pMtppymONc"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = SkipGram_pair(window_size, word_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW_Jn9RO0kTz"
      },
      "source": [
        "- DataLoader 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kkB9XA4izyWC"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2SFUeO-mxiz"
      },
      "source": [
        "# 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdR4u3B3UCa"
      },
      "source": [
        "## 3-1. Skip-gram Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "DXezqWVG3WPl"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module) :\n",
        "\n",
        "    def __init__(self, vocab_size, dimention_size) :\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, dimention_size)\n",
        "        self.linear = nn.Linear(dimention_size, vocab_size, bias = False)\n",
        "        self.activation = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, X) :\n",
        "        X = self.embeddings(X)\n",
        "        X = self.linear(X)\n",
        "        X = self.activation(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHTs5U3T9xC4"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6unIZ9LSLLOu"
      },
      "source": [
        "## 4-1. Skip-Gram Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y5howjC5i5O"
      },
      "source": [
        "- parameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "NKRQ6CeSH3LM"
      },
      "outputs": [],
      "source": [
        "dimension_size = 10\n",
        "epochs = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjwnRVzo5oqv"
      },
      "source": [
        "- Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx22jBOsIEBl",
        "outputId": "e41a5a3e-3e18-4185-da9c-deea639b630e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0, loss : 3.467\n",
            "epoch : 100, loss : 2.753\n",
            "epoch : 200, loss : 2.497\n",
            "epoch : 300, loss : 2.499\n",
            "epoch : 400, loss : 1.796\n",
            "epoch : 500, loss : 1.778\n",
            "epoch : 600, loss : 1.821\n",
            "epoch : 700, loss : 1.662\n",
            "epoch : 800, loss : 1.636\n",
            "epoch : 900, loss : 2.296\n",
            "epoch : 1000, loss : 1.238\n"
          ]
        }
      ],
      "source": [
        "model = SkipGram(vocab_size, dimension_size)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(epochs+1) :\n",
        "    for feature, label in train_dataloader :\n",
        "        out = model(feature)\n",
        "        loss = criterion(out, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 100 == 0 : print(\"epoch : {:d}, loss : {:0.3f}\".format(i, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F1QxoEP9pej"
      },
      "source": [
        "# 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "zclsKnpNgDnm"
      },
      "outputs": [],
      "source": [
        "index2word = {value : key for key, value in word2index.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gDKTb59rI6"
      },
      "source": [
        "## 5-1. 단어의 유사도 측정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65QFGPj6K9CP"
      },
      "source": [
        "### 5-1-1. 유사 단어 상위 3개 추출 함수 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rpAcrRPpgLel"
      },
      "outputs": [],
      "source": [
        "def find_similarity(target_word) :\n",
        "    target_word_embed = model.state_dict()['embeddings.weight'][word2index[target_word]]\n",
        "\n",
        "    similarity = []\n",
        "    for i in range(len(word2index)):\n",
        "        if target_word != index2word[i]:\n",
        "            similarity.append(( i, F.cosine_similarity(target_word_embed.unsqueeze(0), model.state_dict()['embeddings.weight'][i].unsqueeze(0)).item()))\n",
        "        else:\n",
        "            similarity.append((i, -1)) # target_word와 동일 단어는 -1 처리\n",
        "\n",
        "    # 유사도 내림차순 정렬\n",
        "    similarity.sort(key = lambda x : -x[1])\n",
        "\n",
        "    # 인덱스를 단어로 변환\n",
        "    print(f'{target_word}와 유사한 단어:')\n",
        "    for i in range(3) :\n",
        "        print(f'{i+1}위 : {index2word[similarity[i][0]]}({similarity[i][1]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfv54a5p0_4B"
      },
      "source": [
        "### 5-2-2. 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS0bkEr9gQlH",
        "outputId": "f2931e4c-de4c-4770-ddec-ba190314a286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cola와 유사한 단어:\n",
            "1위 : juice(0.5467252731323242)\n",
            "2위 : apple(0.40455394983291626)\n",
            "3위 : is(0.3187265992164612)\n"
          ]
        }
      ],
      "source": [
        "find_similarity(\"cola\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
